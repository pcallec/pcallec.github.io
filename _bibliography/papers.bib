---
---

@article{chen2022b,
  abbr={Sci. Rep.},
  bibtex_show={true},
  author = {Wang*, Chen and Calle*, Paul and Reynolds, Justin C. and Ton, Sam and Yan, Feng and Donaldson, Anthony M. and Ladymon, Avery D. and Roberts, Pamela R. and de Armendi, Alberto J. and Fung, Kar-ming and Shettar, Shashank S. and Pan, Chongle and Tang, Qinggong},
  title = {Epidural anesthesia needle guidance by forward-view endoscopic optical coherence tomography and deep learning},
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {9057},
  abstract = {Epidural anesthesia requires injection of anesthetic into the epidural space in the spine. Accurate placement of the epidural needle is a major challenge. To address this, we developed a forward-view endoscopic optical coherence tomography (OCT) system for real-time imaging of the tissue in front of the needle tip during the puncture. We tested this OCT system in porcine backbones and developed a set of deep learning models to automatically process the imaging data for needle localization. A series of binary classification models were developed to recognize the five layers of the backbone, including fat, interspinous ligament, ligamentum flavum, epidural space, and spinal cord. The classification models provided an average classification accuracy of 96.65%. During puncture, it is important to maintain a safe distance between the needle tip and the dura mater. Regression models were developed to estimate that distance based on the OCT imaging data. Based on the Inception architecture, our models achieved a mean absolute percentage error of 3.05% ± 0.55%. Overall, our results validated the technical feasibility of using this novel imaging strategy to automatically recognize different tissue structures and measure the distances ahead of the needle tip during the epidural needle placement.},
  ISSN = {2045-2322},
  DOI = {10.1038/s41598-022-12950-7},
  url = {https://doi.org/10.1038/s41598-022-12950-7},
  year = {2022},
  code={https://github.com/thepanlab/Endoscopic_OCT_Epidural},
  html={https://www.nature.com/articles/s41598-022-12950-7},
  selected={true}
}

@article{chen2021,
  abbr={BOE},
  bibtex_show={true},
  author = {Wang*, Chen and Calle*, Paul and Tran Ton, Nu Bao and Zhang, Zuyuan and Yan, Feng and Donaldson, Anthony M. and Bradley, Nathan A. and Yu, Zhongxin and Fung, Kar-ming and Pan, Chongle and Tang, Qinggong},
  title = {Deep-learning-aided forward optical coherence tomography endoscope for percutaneous nephrostomy guidance},
  journal = {Biomedical Optics Express},
  volume = {12},
  number = {4},
  pages = {2404-2418},
  abstract = {Percutaneous renal access is the critical initial step in many medical settings. In order to obtain the best surgical outcome with minimum patient morbidity, an improved method for access to the renal calyx is needed. In our study, we built a forward-view optical coherence tomography (OCT) endoscopic system for percutaneous nephrostomy (PCN) guidance. Porcine kidneys were imaged in our experiment to demonstrate the feasibility of the imaging system. Three tissue types of porcine kidneys (renal cortex, medulla, and calyx) can be clearly distinguished due to the morphological and tissue differences from the OCT endoscopic images. To further improve the guidance efficacy and reduce the learning burden of the clinical doctors, a deep-learning-based computer aided diagnosis platform was developed to automatically classify the OCT images by the renal tissue types. Convolutional neural networks (CNN) were developed with labeled OCT images based on the ResNet34, MobileNetv2 and ResNet50 architectures. Nested cross-validation and testing was used to benchmark the classification performance with uncertainty quantification over 10 kidneys, which demonstrated robust performance over substantial biological variability among kidneys. ResNet50-based CNN models achieved an average classification accuracy of 82.6&#x0025;&#x00B1;3.0&#x0025;. The classification precisions were 79&#x0025;&#x00B1;4&#x0025; for cortex, 85&#x0025;&#x00B1;6&#x0025; for medulla, and 91&#x0025;&#x00B1;5&#x0025; for calyx and the classification recalls were 68&#x0025;&#x00B1;11&#x0025; for cortex, 91&#x0025;&#x00B1;4&#x0025; for medulla, and 89&#x0025;&#x00B1;3&#x0025; for calyx. Interpretation of the CNN predictions showed the discriminative characteristics in the OCT images of the three renal tissue types. The results validated the technical feasibility of using this novel imaging platform to automatically recognize the images of renal tissue structures ahead of the PCN needle in PCN surgery.},
  keywords = {Endoscopic imaging
  In vivo imaging
  Laser scanning
  Medical imaging
  Optical coherence tomography
  Three dimensional imaging},
  DOI = {10.1364/BOE.421299},
  url = {https://opg.optica.org/boe/abstract.cfm?URI=boe-12-4-2404},
  year = {2021},
  code={https://github.com/thepanlab/FOCT_kidney},
  html={https://opg.optica.org/boe/fulltext.cfm?uri=boe-12-4-2404},
  selected={true}
}

@article{chen2022a,
  abbr={J. Biophotonics},
  bibtex_show={true},
  author = {Wang, Chen and Reynolds, Justin C. and Calle, Paul and Ladymon, Avery D. and Yan, Feng and Yan, Yuyang and Ton, Sam and Fung, Kar-ming and Patel, Sanjay G. and Yu, Zhongxin and Pan, Chongle and Tang, Qinggong},
  title = {Computer-aided Veress needle guidance using endoscopic optical coherence tomography and convolutional neural networks},
  journal = {Journal of Biophotonics},
  volume = {15},
  number = {5},
  pages = {e202100347},
  abstract = {Abstract During laparoscopic surgery, the Veress needle is commonly used in pneumoperitoneum establishment. Precise placement of the Veress needle is still a challenge for the surgeon. In this study, a computer-aided endoscopic optical coherence tomography (OCT) system was developed to effectively and safely guide Veress needle insertion. This endoscopic system was tested by imaging subcutaneous fat, muscle, abdominal space, and the small intestine from swine samples to simulate the surgical process, including the situation with small intestine injury. Each tissue layer was visualized in OCT images with unique features and subsequently used to develop a system for automatic localization of the Veress needle tip by identifying tissue layers (or spaces) and estimating the needle-to-tissue distance. We used convolutional neural networks (CNNs) in automatic tissue classification and distance estimation. The average testing accuracy in tissue classification was 98.53 ± 0.39%, and the average testing relative error in distance estimation reached 4.42 ± 0.56% (36.09 ± 4.92 μm).},
  ISSN = {1864-063X},
  DOI = {https://doi.org/10.1002/jbio.202100347},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jbio.202100347},
  year = {2022},
}
